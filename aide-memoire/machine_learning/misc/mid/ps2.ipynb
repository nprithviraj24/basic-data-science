{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Problem Set #2: Discriminative and Generative Classifiers\n",
    "\n",
    "In this problem set, you will explore discriminative and generative classifiers.\n",
    "\n",
    "## Problem 1: Generate an interesting data set\n",
    "\n",
    "In class, we considered a synthetic problem in which a training data set was sampled from two classes. In this problem, you will generate a data set with similar characteristics to the dataset discussed in class.\n",
    "\n",
    "**Class 1:** Two features $x_1$ and $x_2$ jointly distributed as a two-dimensional spherical Gaussian with parameters\n",
    "\n",
    "$$\\mu = \\begin{bmatrix} x_{1c} \\\\ x_{2c} \\end{bmatrix},\n",
    "\\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_1^2 \\end{bmatrix}.$$\n",
    "\n",
    "**Class 2:** Two features $x_1$ and $x_2$ in which the data are generated by first sampling an angle $\\theta$ according to a uniform distribution, sampling a distance $d$ according to a one-dimensional Gaussian with a mean of $(3\\sigma_1)^2$ and a variance of $(\\frac{1}{2}\\sigma_1)^2$, then outputting the point $$\\textbf{x} = \\begin{bmatrix} x_{1c} + d \\cos\\theta \\\\ x_{2c} + d \\sin\\theta \\end{bmatrix}$$.\n",
    "\n",
    "Place your code to generate 100 samples from the each of the classes and plot them in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sig1 = 1\n",
    "\n",
    "# mean = [2,2]\n",
    "# cov=[[1,0],[0,1]]\n",
    "# mean2 = [9,9]\n",
    "# cov2 = [[0.25,0], [0,0.25]]\n",
    "\n",
    "# t=360\n",
    "\n",
    "# c1x1 ,c1x2 = np.random.multivariate_normal(mean, cov, 100 ).T\n",
    "# c2x1, c2x2 = np.random.multivariate_normal(mean, cov2, 100 ).T\n",
    "# c2x1 += *np.cos(2*np.pi * t)\n",
    "# c2x2 += 2*np.sin(2*np.pi * t)\n",
    "\n",
    "# plt.plot(c1x1, c1x2, 'r.')\n",
    "\n",
    "# plt.plot(c2x1, c2x2, 'b.')\n",
    "# plt.show()\n",
    "\n",
    "mu_1 = np.array([1.0, 3.0])\n",
    "sigma_1 = 1\n",
    "num_sample = 100\n",
    "cov_mat = np.matrix([[sigma_1,0],[0,sigma_1]])\n",
    "X1 = np.random.multivariate_normal(mean= mu_1, cov=cov_mat, size = num_sample)\n",
    "\n",
    "angle = np.random.uniform(0, 2*np.pi, num_sample)\n",
    "d =  np.random.normal(np.square(3*sigma_1),np.square(.5*sigma_1),num_sample)\n",
    "\n",
    "X2 = np.matrix([X1[:,0] + d*np.cos(angle), X1[:,1] + d*np.sin(angle)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Discriminative classification with logistic regression\n",
    "\n",
    "Split the dataset into 80 patterns per class for training and 20 patterns per class for\n",
    "validation/testing.\n",
    "\n",
    "Perform three experiments with logistic regression: batch gradient ascent on the log likelihood,\n",
    "stochastic gradient ascent on the log likelihood, and batch Newton's method.\n",
    "\n",
    "For each method, plot log likelihood and classification accuracy on the test set and\n",
    "training set as a function of iteration (one batch or pass through the training set per iteration).\n",
    "\n",
    "After showing your code and results, in the cell(s) below, briefly discuss your results. Are all\n",
    "three methods converging to the same maximum? Is the Hessian always invertible? Plot the test data\n",
    "and decision boundary for at least one of the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.r_[X1,X2]\n",
    "Y = np.array([np.r_[np.zeros(len(X1)), np.ones(len(X2))]]).T\n",
    "X.shape\n",
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train = np.c_[X_train, y_train]\n",
    "# train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Transformation of the feature space\n",
    "\n",
    "Perform a polar transform on the data and re-run one of your logistic regression models (whichever you prefer).\n",
    "As before, plot the log likelihood and accuracy on the training set and test set as a function of iteration.\n",
    "\n",
    "Comment on your results. Obviously, this is an example contrived to illustrate the importance of data representation\n",
    "and how much of a difference a simple transformation of the feature space can make. But can you think of some\n",
    "real world problems that would be similarly difficult?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Derive maximum likelihood parameter estimation method\n",
    "\n",
    "We already know that the maximum likelihood estimates of the mean and covariance of the\n",
    "Gaussian-distributed class are the mean and covariance of the sample. But let's derive\n",
    "a maximum likelihood estimator for the parameters of the second class.\n",
    "\n",
    "In class, we outlined a procedure for estimating the parameters $x_{1c}$, $x_{2c}$, $r$, and $\\sigma$ of\n",
    "a generative model for points in the annulus shaped class. Complete the exercise. What are the maximum likelihood\n",
    "estimates of the four parameters for a particular data set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Generative classifier\n",
    "\n",
    "Based on the parameter estimation method you derived in Problem 4, build a maximum a posteriori classifier for the data based on the generative model\n",
    "$$p(y \\mid \\textbf{x}) \\propto p(\\textbf{x} \\mid y) p(y).$$\n",
    "Show your results and compare to the results of Problems 2 and 3. Which approach is best for this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
